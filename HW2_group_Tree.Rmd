---
title: "Predictive Analysis Project - Freemium To Premium Subscribers using Naive Bayes, Decision Tree, and Logistic Regression"
author: "Allison Liu"
date: "2023-07-16"
output: html_document
---
| Workflow of building predictive models - Naive Bayes, Decision Tree, Logistic Regression:
| 1. Import the XYZData.csv
| 2. Clean data and EDA
| 3. Split data into training and testing and oversampling
| 4. Build the model 
| 5. Make prediction
| 6. Measure performance
| 7. Model tuning


## Input XYZData
- Load XYZData and summarize the data
```{r}
library(dplyr)
library(pROC)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(tidymodels)
library(tidyr)
library(kknn)
library(ROSE)
library(randomForest)

XYZData = read.csv("XYZData.csv", stringsAsFactors = TRUE)[ ,2:27]
head(XYZData, 20)

#Convert to factor level
#Factors<-c("male","good_country", "adopter")
#XYZData[Factors]<-lapply(XYZData[Factors],factor)
summary(XYZData)
```

## EDA before analyzing
| 1. We want to check the proportion of outcome variable - adopter.
| 2. We draw histograms for all numeric variables in one plot to understand the data distribution.
```{r}
#Plot Histograms of Numeric Variables in the XYZDataset
# Prepare the dataset for ggplot2
XYZ_data_long <- XYZData %>%
 pivot_longer(cols = c(age, friend_cnt, avg_friend_age, avg_friend_male, friend_country_cnt, subscriber_friend_cnt, songsListened, lovedTracks, posts, playlists, shouts, tenure),
              names_to = "variable",
              values_to = "value")
# Create a histogram for all numeric variables in one plot
XYZ_histograms <- ggplot(XYZ_data_long, aes(x = value)) +
 geom_histogram(bins = 30, color = "black", fill = "lightblue") +
 facet_wrap(~variable, scales = "free", ncol = 4) +
 labs(title = "Histograms of Numeric Variables in the XYZDataset",
      x = "Value",
      y = "Frequency") +
 theme_minimal()

# Plot the histograms
print(XYZ_histograms)

par(mfrow = c(1, 2))
# pie chart for region
pie(table(XYZData$adopter), labels = round(table(XYZData$adopter)/41540, 2), main ="Adopter Pie Chart", col = rainbow(3))
legend("topright", c("0","1"), cex = 0.8, fill = rainbow(3))

```


## Split data into 70% training data and 30% testing data
- We split data to avoid models overfitting.
```{r}

# Randomly pick the rows for training partition
set.seed(123)
train_rows = createDataPartition(y = XYZData$adopter, p = 0.70, list = FALSE)
XYZData_train = XYZData[train_rows,]
XYZData_test = XYZData[-train_rows,]

table(XYZData_train$adopter)
table(XYZData_test$adopter)
```


## Oversampling the Minority Class
| According to the confusion matrix and AUC, this decision tree model indeed had bad precision and recall, which are indexes that we care about in the marketing campaign - predicting customers converting from free users to premium ones.
| Thus, we did oversampling to make up for the problem of too little data for adopter = 1.
| We multiply the amount of data with adopter = 0 by 1.2

```{r}
#set.seed(123)
train_balanced_over <- ovun.sample(adopter ~ ., data = XYZData_train, method = "over",N = 33600)$data
table(train_balanced_over$adopter)
```

## Features Selection - Information Gain
We can use features selection to know which variables are more important and highly relevant to decision variable - adopter.
```{r}
library(FSelectorRcpp)
IG = information_gain(adopter ~ ., data = train_balanced_over)
# e.g., select top 5
topK = cut_attrs(IG, k = 10)
XYZ_topK_train = train_balanced_over %>% select(topK, adopter)
XYZ_topK_test = XYZData_test %>% select(topK, adopter)
XYZ_topK_train$adopter <- factor(XYZ_topK_train$adopter)
XYZ_topK_test$adopter <- factor(XYZ_topK_test$adopter)
# Then you can proceed with model training / testing as usual.

```

## Build models
| 1. Naive Bayes 
| 2. Decision Tree
| 3. Logistic Regression

## 1. Train a Naive Bayes Model
First of all, we build Naive Bayes Model to evaluate majority voting method.
```{r}
library(e1071)
library(caret)
NB_model = naiveBayes(adopter ~ ., data = train_balanced_over, usekernel = T)
NB_model
# Make Predictions
pred_nb = predict(NB_model, XYZData_test)
prob_pred_nb = predict(NB_model, XYZData_test, type = "raw")
XYZData_test$adopter <- factor(XYZData_test$adopter)
confusionMatrix(data = pred_nb,
                reference = XYZData_test$adopter,
                mode = "prec_recall", positive = "1")
```

## Make ROC and calculate AUC
```{r}
XYZ_test_roc = XYZData_test %>%
mutate(prob = prob_pred_nb[,"1"]) %>%
arrange(desc(prob)) %>%
mutate(yes = ifelse(adopter=="1",1,0)) %>%
# the following two lines make the roc curve
mutate(TPR = cumsum(yes)/sum(yes),
FPR = cumsum(1-yes)/sum(1-yes))

ggplot(data = XYZ_test_roc, aes(x = FPR, y = TPR)) +
geom_line() +
theme_bw()
# AUC
roc_nb = roc(response = XYZ_test_roc$yes,
predictor = XYZ_test_roc$prob)
auc(roc_nb)
```

## 2. Train a Decision Tree Model 
| We create a loop to use **Cross-Validation, 5 folds**, to train and test model. 
| According to different folds, we draw corresponding confusion matrix, ROC, AUC, and CP plots to evaluate our models' performance.
```{r}
# Load required libraries
library(ROSE)
library(rpart)
library(caret)
library(pROC)
library(rpart.plot)

# Define the number of folds for k-fold cross-validation
num_folds <- 5
cv <- createFolds(XYZData$adopter, k = num_folds, list = TRUE, returnTrain = FALSE)
# Create empty lists to store results
confusion_matrices <- list()
auc_scores <- numeric(num_folds)
trees <- list()

# Loop through each fold
for (i in 1:num_folds) {
  test_rows <- cv[[i]]  # Get the test data indices for the current fold
  
  XYZ_train <- XYZData[-test_rows, ]
  XYZ_test <- XYZData[test_rows, ]
  table(XYZ_train$adopter)
  
  # Oversampling the training data - We set the total training data = 1.2(XYZ_train$adopter = 1)
  XYZ_over_train <- ovun.sample(adopter ~ ., data = XYZ_train, method = "over", N = 38400)$data
  table(XYZ_over_train$adopter)
  
  # Get the training and testing data for the current fold
  train_indices <- unlist(cv[-i])
  test_indices <- cv[[i]]
  
  train_data <- XYZ_over_train[train_indices, ]
  test_data <- XYZ_over_train[test_indices, ]
  
  # Train the model (Decision Tree)
  tree <- rpart(adopter ~ ., data = train_data,
                method = "class",
                parms = list(split = "information"),
                control = rpart.control(cp = 0.0005, minsplit = 10, maxdepth = 5, minbucket = round(20/3)))
  
  # Make predictions on the test data
  pred_tree <- predict(tree, test_data, type = "class")
  test_data$adopter <- factor(test_data$adopter)
  
  # Evaluate performance using Confusion Matrix
  cm <- confusionMatrix(data = pred_tree,
                        reference = test_data$adopter,
                        mode = "prec_recall", positive = "1")
  
  # Store the confusion matrix in the list
  confusion_matrices[[i]] <- cm
  
  # Evaluate performance using ROC Curve and AUC
  pred_tree_roc <- predict(tree, test_data, type = "prob")
  roc_tree <- roc(response = test_data$adopter,
                  predictor = pred_tree_roc[, "1"])
  
  # Plot the ROC curve
  plot(roc_tree)
  
  # Calculate the AUC score
  auc_score <- auc(roc_tree)
  
  # Store the AUC score
  auc_scores[i] <- auc_score
  
  # Store the tree in the list
  trees[[i]] <- tree
  
  # Print the fold number and the corresponding confusion matrix
  cat("Fold", i, "Confusion Matrix:\n")
  print(cm)
  printcp(x = tree) 
  plotcp(x = tree)
}

# Print the AUC scores for each fold
cat("AUC Scores:", auc_scores, "\n")

# Plot the trees
for (i in 1:length(trees)) {
  cat("Tree for Fold", i, ":\n")
  rpart.plot(trees[[i]], varlen = 0, type = 4, extra = 101, under = TRUE, cex = 0.6, box.palette = "auto")
  cat("\n")
}

```


## Importance variables
According to decision tree, creating the variables' importance graph.
```{r}
# Load the necessary library
library(vip)

# Create a variable importance plot
var_importance <- vip::vip(tree, num_features = 10)
print(var_importance)
```

|**Decision Tree Models Parameter Tuning** 
| We want to modify parameter to get a better model.
| cp = 0.00025, minsplit = 35, maxdepth = 6
```{r}
set.seed(123)
# Define the number of folds for k-fold cross-validation
num_folds <- 5
cv <- createFolds(XYZData$adopter, k = num_folds, list = TRUE, returnTrain = FALSE)
# Create empty lists to store results
confusion_matrices <- list()
auc_scores <- numeric(num_folds)
trees <- list()

# Loop through each fold
for (i in 1:num_folds) {
  test_rows <- cv[[i]]  # Get the test data indices for the current fold
  
  XYZ_train <- XYZData[-test_rows, ]
  XYZ_test <- XYZData[test_rows, ]
  table(XYZ_train$adopter)
  
  # Oversampling the training data - We set the total training data = 1.2(XYZ_train$adopter = 1)
  XYZ_over_train <- ovun.sample(adopter ~ ., data = XYZ_train, method = "over", N = 38400)$data
  table(XYZ_over_train$adopter)
  
  # Get the training and testing data for the current fold
  train_indices <- unlist(cv[-i])
  test_indices <- cv[[i]]
  
  train_data <- XYZ_over_train[train_indices, ]
  test_data <- XYZ_over_train[test_indices, ]
  
  # Train the model (Decision Tree)
  tree2 <- rpart(adopter ~ ., data = train_data,
                method = "class",
                parms = list(split = "gini"),
                control = rpart.control(cp = 0.00025, minsplit = 35, maxdepth = 6, minbucket = round(10)))
  
  # Make predictions on the test data
  pred_tree <- predict(tree2, test_data, type = "class")
  test_data$adopter <- factor(test_data$adopter)
  
  # Evaluate performance using Confusion Matrix
  cm <- confusionMatrix(data = pred_tree,
                        reference = test_data$adopter,
                        mode = "prec_recall", positive = "1")
  
  # Store the confusion matrix in the list
  confusion_matrices[[i]] <- cm
  
  # Evaluate performance using ROC Curve and AUC
  pred_tree_roc <- predict(tree2, test_data, type = "prob")
  roc_tree <- roc(response = test_data$adopter,
                  predictor = pred_tree_roc[, "1"])
  
  # Plot the ROC curve
  plot(roc_tree)
  
  # Calculate the AUC score
  auc_score <- auc(roc_tree)
  
  # Store the AUC score
  auc_scores[i] <- auc_score
  
  # Store the tree in the list
  trees[[i]] <- tree2
  
  # Print the fold number and the corresponding confusion matrix
  cat("Fold", i, "Confusion Matrix:\n")
  print(cm)
  printcp(x = tree2) 
  plotcp(x = tree2)
  
  # Make ROC Curve for adopter = 1
  XYZ_test_cr = test_data %>%
    mutate(prob = pred_tree_roc[,"1"]) %>%
    arrange(desc(prob)) %>%
    mutate(adp = ifelse(test_data$adopter=="1",1,0)) %>%
  # the following two lines make the cumulative response curve
    mutate(y = cumsum(adp)/sum(adp),
           x = row_number()/nrow(test_data))
  # Then, simply plot it.
  ggplot(data = XYZ_test_cr, aes(x = x, y = y)) +
  geom_line() +
  theme_bw()
}

# Print the AUC scores for each fold
cat("AUC Scores:", auc_scores, "\n")

# Plot the trees
for (i in 1:length(trees)) {
  cat("Tree for Fold", i, ":\n")
  rpart.plot(trees[[i]], varlen = 0, type = 4, extra = 101, under = TRUE, cex = 0.6, box.palette = "auto")
  cat("\n")
}

```
We found that when maxdepth increased, we can get a better precision and recall, but the tree will be more complex and hard-to-read.

Reference: https://www.guru99.com/r-decision-trees.html#2
https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/#:~:text=2.-,Oversampling,Random%20Oversampling%20and%20Informative%20Oversampling.


## 3. Train a Logistic Regression Model & Cross-Validation
According result variable importance above, we choose 
```{r}
# Define the number of folds for k-fold cross-validation
XYZ_topK = XYZData[c(3, 4, 6, 7, 8, 9, 14, 18, 26)]
num_folds <- 5
cv <- createFolds(XYZ_topK$adopter, k = num_folds, list = TRUE, returnTrain = FALSE)
# Create empty lists to store results
confusion_matrices <- list()
auc_scores <- numeric(num_folds)

# Loop through each fold
for (i in 1:num_folds) {
  test_rows <- cv[[i]]  # Get the test data indices for the current fold
  
  XYZ_train <- XYZ_topK[-test_rows, ]
  XYZ_test <- XYZ_topK[test_rows, ]
  #table(XYZ_topK$adopter)
  
  # Oversampling the training data - We set the total training data = 1.2(XYZ_train$adopter = 1)
  XYZ_over_train <- ovun.sample(adopter ~ ., data = XYZ_train, method = "over", N = 38400)$data
  #table(XYZ_over_train$adopter)
  
  # Get the training and testing data for the current fold
  train_indices <- unlist(cv[-i])
  test_indices <- cv[[i]]
  
  train_data <- XYZ_over_train[train_indices, ]
  test_data <- XYZ_over_train[test_indices, ]
  
  # Train the model (Logistic Regression)
  train_data$adopter <- factor(train_data$adopter)

  mylogit <- glm(adopter ~., data = train_data, family = "binomial")
  
  # Make predictions on the test data
  pred_log = predict(mylogit, test_data, type = "response")
  pred_cat <- ifelse(pred_log >= 0.5, 1, 0)

  # Evaluate performance using Confusion Matrix
  cm = confusionMatrix(factor(pred_cat), factor(test_data$adopter), 
                  mode = "prec_recall", positive = '1')
  
  # Store the confusion matrix in the list
  confusion_matrices[[i]] <- cm
  
  # Evaluate performance using ROC Curve and AUC
  pred_log_roc = predict(mylogit, XYZData_test, type = "response")
  roc_log = roc(response = XYZData_test$adopter,
               predictor = pred_log_roc)
  
  # Plot the ROC curve
  plot(roc_log)
  
  # Calculate the AUC score and Store it
  auc_score <- auc(roc_log)
  auc_scores[i] <- auc_score
  
  
  # Print the fold number and the corresponding confusion matrix
  cat("Fold", i, "Confusion Matrix:\n")
  print(cm)
}

# Print the AUC scores for each fold
cat("AUC Scores:", auc_scores, "\n")
summary(mylogit)

```



## Importance variables
```{r}
# Load the necessary library
library(vip)

# Create a variable importance plot
var_importance <- vip::vip(mylogit, num_features = 10)
print(var_importance)
```
